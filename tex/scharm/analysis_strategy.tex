
%% Physics analysis within ATLAS is organized according to a branching hierarchy. At the top level, analyses are divided according to the major components of the LHC physics program. These major working groups operate with relative autonomy and set their own standards for publications. Analyses generated by the ``Standard Model'' group focus on measurements of known quantities, and thus strive for transparency and to minimize elaborate fits and model-dependent conclusions. On the other end of spectrum, ``Exotics'' analyses use a variety of innovative techniques to control for Standard Model backgrounds, and test hypothetical models with both Bayesian and Frequentest statistics. The ``Higgs'' group follows a variety of analysis strategies, but in the interpenetration of results the frequentest dogma reigns supreme. The SUSY group opts for a strategy similar to that in the Higgs group---statistical interpretations are strictly regulated, and the use of a single set of statistical tools is obligatory. While this approach stifles innovation, and shoehorns many searches into confusing and opaque statistical methodology, it does have one advantage: all SUSY results present final results in roughly the same format.

The general strategy \atlas\ SUSY searches has been standardized for analyses of 2012 data. As a result, a substantial fraction of the nomenclature and tools are common between most searches. These general aspects of SUSY searches are discussed here.

\subsection{Strategy for all ATLAS SUSY searches}

The lowest-level unit of a SUSY analysis is the \emph{region}, which is defined by a series of selection criteria or \emph{cuts} on the full 2012 dataset. These cuts may simply reject questionable or ill-understood data, e.g. in cases where parts of the detector are offline or otherwise misbehaving, or they may be designed to select a subset of events especially pure in one physical process. The regions are chosen to be statistically disjoint. Several sub-categories of regions exist:
\begin{description}
\item[signal regions] (SRs) are regions where the hypothetical SUSY \emph{signal} process is enriched: in a searches for particles decaying to dark matter, for example, $\met$ may become large as the event recoils against the invisible particles, and thus the signal region may require large $\met$. In almost all cases, even the harshest cuts are unable to remove all Standard Model backgrounds from the signal region. In these cases, care must be taken to estimate these backgrounds precisely. Before the region definitions are finalized, the signal regions are \emph{blinded} to avoid bias: only simulated events are actually viewed.
\item[control regions] (CRs) are regions where cuts enrich background processes. The role of control regions is to measure backgrounds as precisely as possible in a region where systematic uncertainties are similar to those in the signal region.
\item[validation regions] (VRs) are regions near the signal region with similar background composition, but lacking enriched signal. These regions are used to cross-check the background constraints derived from the control regions.
\end{description}
Regions are populated by \emph{events}, where each event is described by a number of analysis-level variables. These variables may describe individual objects within the event, such as jets or leptons, or they may describe an aggregate of the objects (e.g. the invariant mass of several particles or the total missing transverse energy in the event). The overall \emph{event description} is depicted as the orange box in \cref{fig:analysis-flow}.

Each region is filled twice: once with real data from the detector and once with simulated Standard Model events. With properly normalized backgrounds any new physics will appear as an excess in data events over simulated Standard Model backgrounds.
%% In this idealized scenario, an individual SUSY model is deemed ``more likely'' if the sum of the simulated signal and background is more commensurate with observed data than simulated background alone.
Unfortunately, Standard Model predictions are far from perfect. Rather than rely solely on published measurements of these background processes, searches constrain major backgrounds using control regions. This approach has the added advantage of constraining systematic uncertainties when these uncertainties are similar between control and signal regions. Thus the background prediction, and uncertainty on this prediction, is calculated from an overall fit, in which the backgrounds and signal are normalized to fit with observed data, in the signal and control regions simultaneously.

The fit attempts to maximize likelihood while adjusting a number of \emph{fit parameters}.\footnote{Physicists will often refer to \emph{nuisance parameters}, vaguely defined as those parameters which are of no immediate interest in contrast to \emph{parameters of interest}. This distinction is subjective at best (ATLAS SUSY results don't typically quote \emph{any} of the fit parameters), and can be confusing as many physicists seem to dream up their own definition. To avoid this confusion, the ``nuisance'' modifier will be avoided: parameters used in the fit are simply ``fit parameters''.} Several types of fit parameters exist:
\begin{description}
\item[Free normalization parameters] normalize physical processes. While these parameters are typically near 1, the likelihood pays no penalty when they shift to other values. This ``free floating'' condition effectively places no prior assumptions on the value of normalization parameters. As with any fit, the number of constraints must be equal to or greater than the number of free parameters. Since each additional region adds a constraint, this requirement allows one normalization parameter per region.
\item[Signal normalization] is for most purposes an ordinary normalization parameter, but is treated differently when testing the signal hypothesis. Whereas ordinary background normalization parameters are always allowed to float freely, the signal normalization is constrained to 1 when testing the signal hypothesis.
\item[Systematic effects] are modeled with constrained fit parameters. The constraints themselves are Gaussian. Note that the \emph{coupling} of a systematic to a given physical process is dependent on both the process and the region: for a search with $N$ regions, a systematic which affects to $M$ processes within each region will have $N \times M$ couplings.
\item[Luminosity] falls somewhere between a systematic effect and a normalization parameter. Every simulated process is multiplied by the luminosity, which is constrained to $\lumiunct$.
\end{description}
The exact details of the fit and likelihood function are explained in detail elsewhere~\cite{histfitter}, but the essential concepts can be explained in terms of a simpler toy model.\footnote{The more complicated version of \cref{eq:n-predicted} differs by including a more general expression in place of the $a_{s} k_{spr}$ term, which is asymmetric about $\alpha_{s} = 0$.} The total number of events in one region, $N_r$, is predicted by
\begin{equation}
  N_r = \mathcal{L} \sum_{p} \mu_p \sigma_{p} \epsilon_{pr} \left(1 + \sum_{s} \alpha_s k_{spr}\right)
\label{eq:n-predicted}
\end{equation}
where $\mathcal{L}$ is the luminosity, $\mu$ is the normalization for physical process, $\sigma_{p}$ and $\epsilon_{pr}$ are the nominal cross-section and selection efficiency (based on simulation), $\alpha$ fits the systematic effect, and $k$ couples the systematic parameters to the region and process. The physical processes considered are indexed by $p$, while $r$ indexes the region and $s$ indexes the systematic effects.

Several aspects of \cref{eq:n-predicted} are worthy of further discussion. The $\mathcal{L}$, $\mu$, and $\alpha$ terms are fit parameters which are described by a probability distribution rather than a single fixed value.
%% These distributions are measured \emph{from} the fit, and are not specified beforehand.
Both $\mathcal{L}$ and the $\alpha$'s are constrained. In the case of $\mathcal{L}$, the width of the constraint is the uncertainty in the luminosity. In the case of the $\alpha$'s, these constraints are Gaussian with width 1.
For major background processes, the $\mu$ are allowed to float freely in the fit and are controlled by a designated control region. In the case of minor backgrounds where no control regions is warranted, the normalization is set to one (typically with a large uncertainty).

The values of $\epsilon$ and $k$, by contrast, are derived from simulation and are represented by fixed numbers.
The values of $k$ can be positive or negative. While the actual sign of individual $k$ parameters is unimportant (note that inverting all $\alpha$ and $k$ leaves $N_r$ unchanged), this flexibility coupling the $\alpha$'s to various regions and processes---with different magnitude and even opposite directions---permits more flexibility modeling systematic effects. If, for example, the effect of a given systematic is anti-correlated between two regions, the values of $k$ would have opposite sign.

As mentioned, $\sigma_{p}$ is the cross section of the process and $\epsilon_{pr}$ is the efficiency for that process and region.
The efficiency is defined as $\epsilon_{pr} = N_{pr}^{\rm pass} / N^{\rm total}_{p}$, where $N^{\rm pass}_{pr}$ is the number of events which pass the region requirements and $N^{\rm total}_p$ is the total number of simulated events. The values of $k_{spr}$ are computed according to $k_{spr} = (N^{\text{pass}}_{spr} - N^{\text{pass}}_{pr}) / N^{\rm total}_{p}$, where $N^{\text{pass}}_{spr}$ is a variation on $N^{\rm pass}_{pr}$ where the systematic $s$ has been applied. The actual application of these systematics will be discussed in \cref{sec:systematics}.

The final figure of merit for any given SUSY model is the so-called ``$\cls$''~\cite{cls} value which is measured from the fit.
%% Very roughly, $\cls$ corresponds to $P(\text{data}| s + b) / P(\text{data}| b)$, where $s$ is the signal model and $b$ is the background model.
Roughly speaking, $\cls$ indicates whether the signal model, when added to the standard model background, is a less likely fit to data than the standard model alone: in the situation where $\cls < 0.05$ signals are generally considered to be excluded. ATLAS experiment continues to quite $\cls$ values because they have several useful properties:
\begin{itemize}
\item In the high-statistics limit, $\cls$ resembles a Bayesian likelihood ratio between signal + backgrounds and background only hypothesis.
\item Being a frequentest method, the $\cls$ disallows the inclusion of priors, which arguably makes $\cls$ more objective.
\item Software packages which compute $\cls$ are widely standardized in ATLAS, which allows physicists to compute $\cls$ without an above-average understanding of statistics.
\end{itemize}

\begin{figure}[h]
  \begin{center}
    \includegraphics[width=0.7\textwidth]{dot/objects.pdf}
    \caption[Analysis Strategy]{Diagram of the information flow through a typical SUSY analysis.}
    \label{fig:analysis-flow}
  \end{center}
\end{figure}

SUSY searches are blinded until region definitions are finalized. In practice, this means the searches proceed in in several stages, as shown in \cref{fig:analysis-flow}. The stages of analysis are listed below.
\begin{description}
\item[Optimization:] The signal region is defined, beginning with basic choices such as the data stream or the number of leptons or jets required, and refining to more specific details. Signal region data, and any data kinematically close to the signal region, is not used at all in this step. Once a sufficiently pure signal region has been defined, control regions are defined for the major backgrounds. The figure of merit in the optimization process ranges from crude measures of significance, such as $s/b$ or $s/\sqrt{s + b}$ where $s$ and $b$ are the number of signal and background events, to full fits with a blinded signal region. Typically analyses are optimized with several signal regions to exclude (assuming no discovery) the largest possible region of SUSY parameter space.
\item[Validation:] Once the signal and control regions are defined, the background model is ``validated'' by unblinding validation regions near the signal region. The analysis is not typically changed beyond this point.
\item[Unblinding:] The signal region data is unveiled. Assuming no excess is observed in data, $\cls$ values are extracted for each signal point, and an exclusion is calculated.
\end{description}


\subsection{Strategy for the $\sctoc$ Search}

A Feynman diagram for the $\sctoc$ process is shown in \cref{fig:sctocfeyn}. We assume that the lightest supersymmetric quark is the $\tilde{c}$ quark. Other scalar quarks are assumed to be well above 1 TeV in mass. In this scenario, $\tilde{c}$ quarks are pair-produced from gluon splitting. The $\tilde{c}$ quarks decay to the lightest supersymmetric particle, which in this case is a neutrilino ($\neut$). After the $c$-quarks hadronize to $c$-jets and the neutrilinos leave the detector, resulting final state which includes two $c$-jets, no leptons, and $\met$.
\begin{figure}
  \begin{center}
    \includegraphics[width=0.5\textwidth]{%
      paper/figures/feynman_diagram/scsc-ccN1N1.pdf}
    \caption{Feynman diagram of the $\sctoc$ process.}
    \label{fig:sctocfeyn}
  \end{center}
\end{figure}
