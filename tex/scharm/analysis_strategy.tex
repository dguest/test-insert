
The search presented in this thesis is a `cut-and-count' analysis; events in the 2012 dataset, which meet certain quality requirements (cuts) and are likely to arise from the $\sctoc$ process, are counted. In this scheme, any excess in data events over the Standard Model expectation is counted as evidence for a new physical process. Unfortunately, this simple principle grows much more complicated in practice. While Standard Model is well-tested, $\cmenergy$ $pp$ collisions are still very new, and as a result predicting Standard Model backgrounds is non-trivial. Extracting a probabilistic interpenetration from the event counts also adds another challenge. \Cref{sec:regions-and-fitting} gives an overview of the techniques used to overcome these challenges, while \cref{sec:sctoc-overview} explains the features unique to the $\sctoc$ analysis. The remaining sections in this chapter elaborate on the $\sctoc$ search.

\subsection{Regions and Fitting}
\label{sec:regions-and-fitting}

The lowest-level unit of a SUSY analysis is the \emph{region}, which is defined by a series of selection criteria or \emph{cuts} on the full 2012 dataset. These cuts may simply reject questionable or ill-understood data, e.g. in cases where parts of the detector are offline or otherwise misbehaving, or they may be designed to select a subset of events especially pure in one physical process. The regions are chosen to be statistically disjoint. Several sub-categories of regions exist:
\begin{description}
\item[signal regions] (SRs) are regions defined such that the hypothetical SUSY \emph{signal} process is enriched: in a searches for particles decaying to dark matter, for example, $\met$ may become large as the event recoils against the invisible particles, and thus the signal region may require large $\met$. In almost all cases, even the harshest cuts are unable to remove all Standard Model backgrounds from the signal region. In these cases, care must be taken to estimate these backgrounds precisely. Before the region definitions are finalized, the signal regions are \emph{blinded} to avoid bias: only simulated events are actually viewed.
\item[control regions] (CRs) are regions defined such that cuts enrich background processes. The role of control regions is to measure backgrounds as precisely as possible in a region where systematic uncertainties are similar to those in the signal region.
\item[validation regions] (VRs) are regions near the signal region with similar background composition, but lacking enriched signal. These regions are used to cross-check the background constraints derived from the control regions.
\end{description}
Regions are populated by \emph{events}, where each event is described by a number of analysis-level variables. These variables may describe individual objects within the event, such as jets or leptons, or they may describe an aggregate of the objects (e.g. the invariant mass of several particles or the total missing transverse energy in the event).
%% The overall \emph{event description} is depicted as the orange box in \cref{fig:analysis-flow}.

Each region is filled several times: real data from the detector, simulated Standard Model events, and simulated signal events are overlaid in each region. With properly normalized backgrounds any new physics will appear as an excess in data events over simulated Standard Model backgrounds.
Unfortunately, Standard Model predictions are far from perfect. Rather than rely solely on published measurements of these background processes, searches constrain major backgrounds using control regions. This approach has the added advantage of constraining systematic uncertainties when these uncertainties are similar between control and signal regions. Thus the background prediction, and uncertainty on this prediction, is calculated from an overall fit, in which the backgrounds and signal are normalized to fit with observed data, in the signal and control regions simultaneously.

The fit attempts to maximize likelihood while adjusting a number of \emph{fit parameters}.\footnote{Physicists will often refer to \emph{nuisance parameters}, vaguely defined as those parameters which are of no immediate interest in contrast to \emph{parameters of interest}. This distinction is subjective at best (ATLAS SUSY results don't typically quote \emph{any} of the fit parameters), and can be confusing as many physicists seem to dream up their own definition. To avoid this confusion, the ``nuisance'' modifier will be avoided: parameters used in the fit are simply ``fit parameters''.} Several types of fit parameters exist:
\begin{description}
\item[Free normalization parameters] normalize physical processes. While these parameters are typically near 1, the likelihood pays no penalty when they shift to other values. This ``free floating'' condition effectively places no prior assumptions on the value of normalization parameters. As with any fit, the number of constraints must be equal to or greater than the number of free parameters. Since each additional region adds a constraint, this requirement allows one normalization parameter per region.
\item[Signal normalization] is for most purposes an ordinary normalization parameter, but is treated differently when testing the signal hypothesis. Whereas ordinary background normalization parameters are always allowed to float freely, the signal normalization is constrained to 1 when testing the signal hypothesis.
\item[Systematic effects] are modeled with constrained fit parameters. The constraints themselves are Gaussian. Note that the \emph{coupling} of a systematic to a given physical process is dependent on both the process and the region: for a search with $N$ regions, a systematic which affects to $M$ processes within each region will have $N \times M$ couplings.
\item[Luminosity] falls somewhere between a systematic effect and a normalization parameter. Every simulated process is multiplied by the luminosity, which is constrained to $\lumiunct$.
\end{description}
The exact details of the fitting software and and likelihood function are explained in detail elsewhere~\cite{histfitter}, but a brief review is given here as well.
%% \footnote{The more complicated version of \cref{eq:n-predicted} differs by including a more general expression in place of the $a_{s} k_{spr}$ term, which is asymmetric about $\alpha_{s} = 0$.}
The total number of events in one region, $N_r$, is predicted by
\begin{equation}
  N_r = \mathcal{L} \sum_{p} \mu_p \sigma_{p} \epsilon_{pr} \prod_{s} f_{spr}(\alpha_s)
\label{eq:n-predicted}
\end{equation}
where $\mathcal{L}$ is the luminosity, $\mu$ is the normalization for physical process, $\sigma_{p}$ and $\epsilon_{pr}$ are the nominal cross-section and selection efficiency (based on simulation), $\alpha$ fits the systematic effect, and the functions $f_{spr}$ couple the systematic parameters $s$ to the process $p$ and region $r$. The physical processes considered are indexed by $p$, while $r$ indexes the region and $s$ indexes the systematic effects.

Several aspects of \cref{eq:n-predicted} are worthy of further discussion. The $\mathcal{L}$, $\mu$, and $\alpha$ terms are fit parameters which are described by a probability distribution rather than a single fixed value.
%% These distributions are measured \emph{from} the fit, and are not specified beforehand.
Both $\mathcal{L}$ and the $\alpha$'s are constrained. In the case of $\mathcal{L}$, the width of the constraint is the uncertainty in the luminosity. In the case of the $\alpha$'s, these constraints are Gaussian with width 1.
For major background processes, the $\mu$ are allowed to float freely in the fit and are controlled by a designated control region. In the case of minor backgrounds where no control regions is warranted, the normalization is set to one (typically with a large uncertainty).

The values of $\epsilon$ and parameters describing $f$, by contrast, are derived from simulation and are represented by fixed numbers.
The efficiency $\epsilon$ is defined as $\epsilon_{pr} = P_{pr} / T_{p}$, where $P_{pr}$ is the number of events which pass the region requirements and $T_p$ is the total number of simulated events. Both $P$ and $T$ are derived from simulation.

Each of the $f_{spr}$ functions couples a parameter which fits the systematic, $\alpha_s$ to a relative variation in the event yield.
Fixed parameters describing these couplings come in the form of an ``up'' and ``down'' variation in the systematic, computed from simulation.
The parameters which describe these variations, $k_{spr}^{\pm}$, are defined for each physical process according to $k_{spr}^{\pm} = P^{\pm}_{spr} / T_{p}$, where the $P^{\pm}_{spr}$ are variations of $P_{pr}$ with up and down systematics $s$ applied.
Details on the derivation of the individual systematics will be discussed in \cref{sec:systematics}.

The $f$ functions\footnote{For the moment the $spr$ indices are suppressed.} are defined such that $f(\alpha = \pm 1) = k^{\pm}$. To make this coupling a continuous function of $\alpha$, a curve is fit to these points.
For values of $\alpha$ outside the $[-1,1]$ interval, the the functions are extrapolated as $f(\alpha) = (k^{\pm})^{|\alpha|}$, choosing $k^-$ or $k^+$ for values of $\alpha$ below and above zero, respectively.
For values where $\alpha$ is in the $[-1,1]$ interval $f(\alpha)$ is defined by a sixth-order polynomial, constrained by the requirements that $f(\alpha = 0) = 1$ and that $f$,$f'$, and $f''$ are continuous at all values of~$\alpha$.
It is important to note that the `$\pm$' in $k^{\pm}$ refers to the systematic variation and not to the size of the $k$ parameters; the $k^+$ are not necessarily larger than the $k^-$. If, for example, the effect of a given systematic is anti-correlated between two regions, the value of $(k^+ - k^-)$ would have opposite sign in the two regions.
%% While the actual sign of individual $k$ parameters is unimportant (note that inverting all $\alpha$ and $k$ leaves $N_r$ unchanged).
This flexibility coupling the $\alpha$'s to various regions and processes---with different magnitude and even opposite directions---permits more flexibility modeling systematic effects.

%% As mentioned, $\sigma_{p}$ is the cross section of the process.

The final figure of merit for any given SUSY model is the so-called ``$\cls$''~\cite{cls} value which is measured from the fit.
%% Very roughly, $\cls$ corresponds to $P(\text{data}| s + b) / P(\text{data}| b)$, where $s$ is the signal model and $b$ is the background model.
Roughly speaking, $\cls$ indicates whether the signal model, when added to the standard model background, is a less likely fit to data than the standard model alone: in the situation where $\cls < 0.05$ signals are generally considered to be excluded. The ATLAS SUSY group quotes $\cls$ values due to an ingrained perception that they possess useful properties:
\begin{itemize}
\item Being a frequentest method, $\cls$ disallows the inclusion of priors, which can prevent debates regarding the appropriate choice of prior.
\item In the high-statistics limit, $\cls$ resembles a Bayesian likelihood ratio between signal + backgrounds and background only hypothesis.
%% \item Software packages which compute $\cls$ are widely standardized in ATLAS, which allows physicists to compute $\cls$ without an above-average understanding of statistics.
\end{itemize}
Beyond these reasons, the choice of $\cls$ is a historical accident.

%% \begin{figure}[h]
%%   \begin{center}
%%     \includegraphics[width=0.7\textwidth]{dot/objects.pdf}
%%     \caption[Analysis Strategy]{Diagram of the information flow through a typical SUSY analysis.}
%%     \label{fig:analysis-flow}
%%   \end{center}
%% \end{figure}

The search was blinded until region definitions are finalized. In practice, this means the search proceed in in several stages.
%% as shown in \cref{fig:analysis-flow}.
The stages of analysis are listed below.
\begin{description}
\item[Optimization:] The signal region is defined, beginning with basic choices such as the data stream or the number of leptons or jets required, and refining to more specific details. Signal region data, and any data kinematically close to the signal region, is not used at all in this step. Once a sufficiently pure signal region has been defined, control regions are defined for the major backgrounds. The figure of merit in the optimization process ranges from crude measures of significance, such as $s/b$ or $s/\sqrt{s + b}$ where $s$ and $b$ are the number of signal and background events, to full fits with a blinded signal region. The analysis is optimized with several signal regions to exclude (assuming no discovery) the largest possible region of SUSY parameter space.
\item[Validation:] Once the signal and control regions are defined, the background model is ``validated'' by unblinding validation regions near the signal region. The analysis is not changed beyond this point.
\item[Unblinding:] The signal region data is unveiled. Assuming no excess is observed in data, $\cls$ values are extracted for each signal point, and an exclusion is calculated.
\end{description}


\subsection{Overview of the $\sctoc$ Search}
\label{sec:sctoc-overview}

A Feynman diagram the $\sctoc$ process is shown in \cref{fig:sctocfeyn}. For the purposes of this search, the lightest supersymmetric quark is assumed to be the $\tilde{c}$ quark. Other scalar quarks are assumed to be well above 1 TeV in mass. In this scenario, $\tilde{c}$ quarks are pair-produced from gluon splitting. The $\tilde{c}$ quarks decay to the lightest supersymmetric particle, which in this case is a neutrilino ($\neut$). After the $c$-quarks hadronize to $c$-jets and the neutrilinos leave the detector, resulting final state which includes two $c$-jets, no leptons, and $\met$.

Many Standard Model processes decay to final states with two jets and $\met$.
The most common of these is multijet production (\cref{fig:feyn-multijets}) where the energy of one jet is not measured correctly in the calorimeter.
This mismeasurement results in $\met$ which is aligned with one of the jets.
In the $\sctoc$ search, most of these events are removed by requiring $\met > 150 \gev$ and an angular separation in $\phi$ between $\vmet$ and the leading three jets of at least 0.4.
Of the remaining multijet events, many are light-flavor and are thus removed by a requiring medium $c$-tags on the leading two jets.

\newcommand{\feyninc}[3]{\scalebox{#1}{\input{misc/feyngen/#2}}\label{#3}}
\begin{figure}
  \begin{center}
  \subfigure[multijet]{\feyninc{1.6}{multijet}{fig:feyn-multijets}}
  \subfigure[$\zjets$]{\feyninc{1.6}{zjets}{fig:feyn-zjets}} \\[1cm]
  \subfigure[$\wjets$]{\feyninc{1.6}{wjets}{fig:feyn-wjets}}
  \subfigure[$\ttbar$]{\feyninc{1.6}{ttbar}{fig:feyn-ttbar}}
  \caption[Feynman diagrams for Standard Model backgrounds]{Feynman diagrams for the main Standard Model backgrounds in the $\sctoc$ search. Legs that result in $\met$ are shown red.
Note that jets produced from multijet production or in association with electroweak processes are drawn as gluons, but could also arise from quark-jets.}
  \end{center}
\end{figure}

Beyond multijet production several electroweak processes are also significant backgrounds.
The most problematic of these is $\zjets$ production shown in \cref{fig:feyn-zjets}. In cases where the $Z$ boson decays to neutrinos a large fraction of the event energy is lost, resulting in large $\met$ which is uncorrelated with the associated jets. As in multijet production, this background consists mostly of light-flavor jets and can be reduced by $c$-tagging. As discussed in \cref{sec:backgrounds}, this background is difficult to remove completely, and is instead estimated carefully with a designated control region.

A second electroweak background arises when $\wjets$ events decay leptonically. The $\sctoc$ signal regions are defined with a lepton veto, so theoretically such events should never be accepted. In practice the lepton is frequently outside the detector acceptance, or is misidentified as a jet (which is especially easy in the case of $\tau$ leptons); because of the momentum carried away by the neutrino these events include sizable $\met$. This background is similar to $\zjets$; it is reduced by $c$-tagging the leading two jets, but cannot be completely removed.

The final major background for the $\sctoc$ search is ``semi-leptonic'' $\ttbar$, in which one $W$ boson decays to a neutrino and a lepton while the other $W$ decays hadronically (see \cref{fig:feyn-ttbar}). As in the case with $\wjets$, the lepton may fall outside the detector acceptance or be misidentified as a jet, resulting in large $\met$ and many jets. Unlike the $\wjets$ and $\zjets$ cases, however, $\ttbar$ decays almost always contain at least two $b$-jets. The $c$-tagging requirement thus removes a large fraction of these events as well.

The signal regions include several additional cuts to purify the signal and remove low-quality data as discussed in \cref{sec:event-clean,sec:sr}. Of primary importance among these variables is $\mct$~\cite{mct}, defined as
\begin{align}
  \mct^2 &\equiv [p_\T(j_1) + p_\T(j_2)]^2 - [\vpt(j_1) - \vpt(j_2)]^2
  \label{eq:mctdef}
\end{align}
where $j_1$ and $j_2$ are the highest and second-highest energy jets, respectively. This quantity has two uses. First, when pair-produced  $\scharm$ decay to $\neut + c$, $j_1$ and $j_2$ are the resulting $c$-jets and $\mct$ has an upper bound:
\begin{align}
  \mct < \frac{m^2_{\scharm} - m^2_{\neut}}{m_{\scharm}}.
  \label{eq:mctlim}
\end{align}
This makes it a useful quantity to constrain the mass any discovered SUSY particles.
The second use is in background discrimination. In the case of $\ttbar$ decays where $j_1$ and $j_2$ are $b$-jets, \cref{eq:mctlim} holds with $\scharm \to t$ and $\neut \to W$, and thus $\mct < 135\,\gev$ for $\ttbar$.
Three signal regions are defined with successively higher requirements on $\mct$: $\mct > 150\,\gev$, $\mct > 200\,\gev$, and $\mct > 250\,\gev$.

