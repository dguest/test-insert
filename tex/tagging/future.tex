JetFitterCharm, while effective, is plagued by several related issues.
First, in $b$-vs-light discrimination JetFitterCharm is inferior to MV1, the baseline $b$-tagger in run 1. This discrepancy is illustrated in \cref{fig:u-rej-roc}, and is surprising given that both are neural nets with roughly the same inputs.
In addition, the neural net training isn't reproducible; retraining on exactly the same dataset may produce performance that's better or worse by several percent.
Both issues stem from the unreliable methods which are typically used to train neural nets.

\begin{figure}
  \begin{center}
    \includegraphics[width=0.7\textwidth]{figures/external/uRejRoc.pdf}
    \caption[Light jet rejection as a function of $b$-tagging efficiency]{Light jet rejection vs $b$-tagging efficiency for several taggers. The MV1 tagger (labeled \plt{mv1}) was the baseline $b$-tagger for Run 1. Both \plt{gaiaAntiU} and \plt{jfcAntiU} (from  GAIA and JetFitterCharm respectively) rely on neural nets with multiple outputs and use the $\log (P_{b} / P_{\text{light}})$ ratio as a discriminant. A final tagger \plt{gaiaGr1}, is tuned to reject both $c$ and light jets, and discriminates with $\log [P_{b}^2 / (P_{c} P_{\text{light}})]$.  The bottom panel shows relative efficiency vs the \plt{gaiaGr1} baseline.}
    \label{fig:u-rej-roc}
  \end{center}
\end{figure}

\subsection{Traditional Neural Networks}
\label{sec:trad-nn}
Traditional signal-processing neural nets are continuous mappings of input vectors $\vect{x}$ onto output vectors $\vect{y}$, where both $\vect{x}$ and $\vect{y}$ can be of arbitrary dimension for a given neural net.
An $N + 1$ layer neural net can be represented as stack of linear transformations $A_{i}$ interleaved with a nonlinear activation function $f$:
\begin{equation}
  \vect{y} = \mathcal{N}(\vect{x}) = A_N f(A_{N -1} \cdots f(A_1 \vect{x} + \vect{b}_1) \cdots + \vect{b}_{N-1} ) + \vect{b}_N
  \label{eq:nn-functional}
\end{equation}
where the $\vect{b}_i$ are \emph{bias} vectors and $f$ operates on each element of its input vector independently.
A typical activation function is the sigmoid $f(x_j) = (1 + e^{-x_j})^{-1}$.
In \cref{eq:nn-functional}, $\vect{x}$ and $\vect{y}$ are referred to as the input and output layers, respectively.
For historical reasons, the elements of $A_i$ are also known as synapse weights, while the elements of $\vect{b}_i$ are known as activation thresholds.
The inputs to each $A_i$ (other than $A_1$) are referred to as \emph{hidden} layers $\vect{z_i}$. To make the hidden layers more explicet \cref{eq:nn-functional} can also be written recursively:
\begin{equation}
  \vect{z}_{i+1} = f(A_{i} \vect{z}_{i} + \vect{b}_{i}),
  \label{eq:recursive-nn}
\end{equation}
where $\vect{y} = \vect{z}_{N+1}$ and $\vect{x} = \vect{z}_{1}$.
In some cases the activation function is omitted for the final layer, as in \cref{eq:nn-functional}

Note that the dimensions of the $A_{i}$ and $\vect{b}_i$ are constrained by the dimensions of the surrounding layers; if $\vect{x}$ has $n$ elements, $A_1$ is an $m \times n$ matrix, $A_{2}$ is an $o \times m$ matrix, $A_{3}$ is $p \times o$, etc.
Thus given some number of inputs and outputs, the neural net is still left with $N - 1$ free dimensions---one for each hidden layer.
The dimensions of the input, output, and hidden layers specify the \emph{topology} of the neural net.
These dimensions typically decrease through successive layers, since the neural net is designed to filter a large number of parameters into a smaller number of categories.
Traditional neural nets can include several hidden layers; those with more layers are described as ``deep''.
Theoretically a deep neural net can fit a continuous function of $\vect{x}$ to arbitrary precision given an appropriate choice of the of $A_i$ and $\vect{b}_i$.

In the case of flavor tagging, the elements of $\vect{x}$ correspond to measurable jet parameters, while the elements $\vect{y}$ correspond to jet flavor probabilities. Elements of $\{(A_i, \vect{b}_i)\}_{i=0}^{N}$ are ``trained'' using inputs with corresponding \emph{target} outputs $\vect{t}$, such that $\mathcal{N}(\vect{x}) \approx \vect{t}$.
Within flavor tagging these targets correspond to jet flavor probabilities, with one element for each flavor.
This training scheme is said to be \emph{supervised} because it requires known target values.

Traditionally, training progresses in several steps using a technique known as error backpropagation. The steps are as follows:
\begin{enumerate}
\item An activation function $f$ and topology are chosen.
\item The neural net is initialized with random synapse weights and activation thresholds.
\item A pattern is selected from a \emph{training sample} which includes the input values $\vect{x}$ and the \emph{target} values $\vect{t}$. The value of $\vect{y}$ is computed using the current $A_i$ and $\vect{b}$ parameters. \label{item:nn-pattern-select}
\item An \emph{cost function} $E(\vect{y},\vect{t})$ quantifies the amount by which the pattern is misclassified.
A particularly simple cost function is $E(\vect{y},\vect{t}) = \sum_{i} (y_i - t_i)^2 / 2$. %The error $\vect{e}$ on $\vect{y}$ is defined as
%% \begin{equation}
%% \end{equation}
The cost function determines an error for each output element.
\item The error is backpropagated to the previous (hidden) layer using the $A_i$ matrix. By recursively applying backpropagation to each successive layer an error can be established for each element in each hidden layer~\cite{luke-thesis}.
\item The values of $A_i$ and $\vect{b}_i$ are updated by moving both in a direction that will decrease the cost function. The size of the update is dictated by the \emph{learning rate}.\label{item:nn-update}
\item Steps~\ref{item:nn-pattern-select} through~\ref{item:nn-update} are repeated until the cost is minimized.
\end{enumerate}

This procedure works well for shallow networks with one or two hidden layers.
In deeper networks the backpropagation procedure begins to break down.
Backpropagation is a useful heuristic, but doesn't guarantee rapid convergence, especially when errors are propagated through several hidden layers.
As a result training can be prohibitively slow on deeper networks.
Furthermore, once the cost function is minimized there's no guarantee that the neural net has found the global minimum.
Autoencoders solve these problems, as discussed in the next section.

\subsection{Autoencoders}

The conceptual key to autoencoders is recognition that hidden layers are more than placeholders in a numeric filtering sequence.
Instead, they are cast as \emph{feature spaces} which describe the characteristics of input patterns in terms of a reduced set of parameters.
In this view, each layer of a neural net is a compression; it reduces the dimension of the input space while preserving the important information.
The next important principle is that feature spaces can be agnostic to the target patterns on which the neural net is trained or, in the flavor-tagging context, agnostic to the jet flavor.
These two ideas motivate a completely new approach to training neural nets, which proceeds as follows:
\begin{enumerate}
\item A three-layer neural net is defined, with the structure
  \begin{equation}
    \tilde{\vect{x}} = D f(A_1 \vect{x} + \vect{b}_1) + \vect{c}.
    \label{eq:autoencoder-1st}
  \end{equation}
  As above, the matrices and bias vectors are initialized randomly. The matrix $A_1$ is an $m \times n$ matrix with $m < n$, such that it reduces the dimension of the input vector.
\item The neural net is trained with the aim that $\tilde{\vect{x}} \approx \vect{x}$, using steps~\ref{item:nn-pattern-select}--\ref{item:nn-update} from the traditional backpropagation above.
At this point it's convenient to rewrite \cref{eq:autoencoder-1st} as
\begin{equation}
    \tilde{\vect{x}} = D \vect{z}_2 + \vect{c}
\end{equation}
using the definition of $\vect{z}_i$ from \cref{eq:recursive-nn}.
The application of $A_1$ and $\vect{b}_1$ describe a (lossy) \emph{encoding} of the input space, such that $\vect{z}_2$ is a lower-dimensional representation of $\vect{x}$.
Applying $D$ and $\vect{c}$ decodes the compressed $\vect{z}_2$ and reconstructs the original inputs. Note that this only requires input patterns; the training sequence is \emph{unsupervised} because no traditional target values are required.

\item Both $D$ and $\vect{c}$ are discarded, and a new neural net is defined:
  \begin{equation}
    \tilde{\vect{z}_2} = D f(A_2 \vect{z_2} + \vect{b}_2) + \vect{c}
    \label{eq:autoencoder-2st}
  \end{equation}
  where $\vect{z}_2 \equiv f(A_1 \vect{x} + \vect{b}_1)$, and $A_2$ is a dimension-reducing transform as before. Again the neural net is trained with backpropagation, ignoring the first layer and focusing only on the second.
\label{item:autoencoder-layer}
\item Step~\ref{item:autoencoder-layer} is repeated as many times as desired. The resulting stack of encoding layers transforms an input into low-dimensional representation.
\item As a result of the unsupervised training, the final feature space is unlikely to be aligned with the actual target values. Within the flavor tagging context jets are clumped in features which may reflect flavor, but which aren't aligned with the flavor basis.
A final \emph{fine tuning} step aligns these features in the correct basis using the full backpropagation sequence described in \cref{sec:trad-nn}.
\end{enumerate}
This training sequence can be orders of magnitude faster and far more robust than traditional backpropagation.
It is therefore able to construct more complicated neural nets which squeeze much more discrimination out of the input variables.

\subsection{Performance Comparisons}

Explicit proof are few and far between in machine learning---what counts as ``proof'' is generally empirical.
In the case of flavor-tagging, the improvement due to autoencoders is demonstrated in simulated data, with various metrics depending on the tagged flavor.
In the performance comparisons that follow, we compare several new taggers: MV1 was the baseline $b$-tagging tool for Run 1 analyses; GAIA is a new autoencoder-based neural network.
Like JetFitterCharm, GAIA includes three outputs, and discriminates signal from background by selecting jets above a $\log(P_{\text{signal}} / P_{\text{background}})$ threshold.

\subsubsection{$b$ tagging}
In the $b$-tagging case, performance is generally quantified with a Receiver Operating Characteristic (ROC) curve, which is produced by plotting light-jet rejection vs $b$-jet efficiency improvement for a range of selections. Each selection is defined by a specific threshold, over which all jets are accepted.
A comparison on ROC curves for various taggers is given in \cref{fig:u-rej-roc}.
Clearly the GAIA's deep architecture outperforms MV1 when configured as an anti-light tagger using the $\log(P_{b}/P_{\text{light}})$ discriminant.

\begin{figure}
  \begin{center}
    \includegraphics[width=0.7\textwidth]{figures/external/cRejRoc.pdf}
    \caption[Charm jet rejection as a function of $b$-tagging efficiency]{Charm jet rejection vs $b$-tagging efficiency for several taggers. The MV1 tagger (labeled \plt{mv1}) was the baseline $b$-tagger for Run 1. Both \plt{gaiaAntiU} and \plt{jfcAntiU} (from  GAIA and JetFitterCharm respectively) rely on neural nets with multiple outputs and use the $\log (P_{b} / P_{\text{light}})$ ratio as a discriminant. A final tagger \plt{gaiaGr1}, is tuned to reject both $c$ and light jets, and discriminates with $\log [P_{b}^2 / (P_{c} P_{\text{light}})]$.  The bottom panel shows relative efficiency vs the \plt{gaiaGr1} baseline.}
    \label{fig:c-rej-roc}
  \end{center}
\end{figure}

Charm jets can also become a major background in a number of searches.
\Cref{fig:c-rej-roc} gives ROC curves for $c$-jet rejection.
In this case the anti-light GAIA configuration performs on par with MV1, which is unsurprising given that the MV1 training ignores $c$-jets and the GAIA anti-light discriminant ignores $P_{c}$.
To reject a higher portion of $c$-jets, GAIA uses a mixed discriminant of the form $\log[P_{b}^2/(P_{\text{light}} P_{c})]$.
This discriminant rejects far more $c$-jets for a given efficiency, at the price of slightly lowered light-jet rejection.

All lifetime-based taggers perform best on jets within a specific $\pt$ range.
Performance increases steadily with $\pt$ up to roughly $100\,\gev$, after which it begins to decrease.
Poor performance at low $\pt$ is expected, since less boosted objects will decay closer to the primary vertex and secondary vertex reconstruction will fail.
The loss in performance at higher pt, by contrast, is more difficult to explain---while tracks are more colinear (and thus less able to form a well-defined vertex) they should also travel further before decaying (and thus have a more apparent displaced vertex).
The loss of high-$\pt$ performance may simply be an artifact of training on $t\bar{t}$ samples: the vast majority of jets in these samples have $\pt < 200\,\gev$.

\begin{figure}
  \includegraphics[width=0.5\textwidth]{figures/external/uRej70_ptbins.pdf}
  \includegraphics[width=0.5\textwidth]{figures/external/cRej70_ptbins.pdf}
  \caption[Rejection vs $\pt$ for $b$-taggers]{Rejection vs $\pt$ for $b$-taggers. Tagging thresholds are adjusted individually for jets in each $\pt$ bin, such that 70\% of jets are tagged in each bin. The MV1 tagger (labeled \plt{mv1}) was the baseline $b$-tagger for Run 1. Both \plt{gaiaAntiU} and \plt{jfcAntiU} (from  GAIA and JetFitterCharm respectively) rely on neural nets with multiple outputs and use the $\log (P_{b} / P_{\text{light}})$ ratio as a discriminant. A final tagger \plt{gaiaGr1}, is tuned to reject both $c$ and light jets, and discriminates with $\log [P_{b}^2 / (P_{c} P_{\text{light}})]$. }
  \label{fig:rej70-ptbins}
\end{figure}

\subsubsection{$c$ tagging}

As discussed in \cref{sec:ctag-perf}, $c$-tagging performance metrics are inherently more complicated than those used in $b$ tagging.
To compare the $c$-tagging performance across multiple taggers we calculate the relative $c$-tagging efficiency at any given $b$ and light rejection, as shown in \cref{fig:ctag-gaia-vs-jfc}.
At any chosen point, GAIA outperforms JetFitterCharm.

\begin{figure}
  \begin{center}
  \includegraphics[width=0.8\textwidth]{%
    figures/external/ctag-2d-gaia-vs-jfc.pdf}
  \caption[Charm jet efficiency comparison in duel-rejection plane]{%
    Relative efficiency of GAIA / JetFitterCharm in the light-vs-$b$ rejection plane. Black thick black contours indicate constant efficiency for GAIA.}
  \label{fig:ctag-gaia-vs-jfc}
  \end{center}
\end{figure}

