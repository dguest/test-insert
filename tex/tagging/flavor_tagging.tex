\chapter{Charm Tagging}

\section{Introduction}
\label{tag:sec:into}

\newcommand{\jfcplotswherefrom}{The jets are from $t \bar{t}$ simulated events generated with \textsc{Powheg+Pythia6}.}
\newcommand{\wherefrom}{The jets are from $t \bar{t}$ simulated events generated with \textsc{Powheg+Pythia6}.}

Final states including $c$-jets can arise from a number of interesting processes at the Large Hadron Collider.
Some of these, including Beyond the Standard Model $H \to c\bar{c}$ production~\cite{charminghiggs} and SUSY models in which the lightest scalar quark decays to $c$-quarks~\cite{stoptocharm}, could be observed in LHC data.
Unfortunately, in the absence of a designated $c$-quark jet ($c$-jet) identification algorithm, such searches are forced to contend with large Standard Model backgrounds: multijet production, $b$-quark jet ($b$-jet) backgrounds from $t\bar{t}$ decays, other electroweak processes, and $H \to b\bar{b}$ production can all overwhelm the $c$-jet signal.
Assuming these backgrounds can be controlled, these searches have little sensitivity to the flavor couplings of a signal.
To address these problems, a dedicated algorithm to identify $c$-jets ($c$-tagging) has been developed and applied to data collected by the ATLAS detector~\cite{DetPap}.

\begin{figure}
  \begin{center}
\includegraphics[width=0.7\textwidth]{figures/external/sm-b-decay.pdf}
\caption[The decay chain of a $b$-quark]{Diagram of a $b$-quark decay though standard model particles. The $b$-quark decays dominantly though a cascade as $b \to c \to s \to u$. Other decays are possible but are heavily CKM suppressed.}
\label{fig:sm-b-decay}
  \end{center}
\end{figure}

For the sake of charm-tagging, a $c$-quark decay is best explained as a component of a $b$-quark decay.
The sequence of a typical $b$-quark decay is shown in \cref{fig:sm-b-decay}.
Once created, a $b$-quark is only kinematically allowed to decay through less massive particles, and overwhelmingly does so through $c$-quarks.
Because this first decay is CKM suppressed, the $b$-hadron travels a substantial distance before decaying, as shown in \cref{fig:b-jet}.
The subsequent charmed-meson decay lacks CKM suppression and is slightly faster\footnote{The CKM-favored decay of the $c$ quark is counteracted by the decay having less avalible phase space, since the $c$--$s$ mass splitting is smaller}.
Both decays are slow enough that the particles fly a measurable distance.
In the case of a $b$-quark with average decay lengths, both the secondary and tertiary vertex can be reconstructed.
A $c$-jet is simply the downstream part of a $b$-jet.
In this sense, every $b$-jet in the LHC also contains a $c$-jet; the jet energy thresholds required by searches are high enough that $b$-jets remain columnated despite the energy released in the $b \to c$ decay.

\begin{figure}
  \begin{center}
    \includegraphics[width=0.7\textwidth]{figures/external/b-jet.pdf}
    \caption[Diagram of a $B$-meson decay.]{Diagram of a $B$ meson decay. The $B$ meson contains a $b$ quark and is thus unstable. At LHC energies, many $B$ mesons travel a measurable distance before decaying. The daughter $D$ meson, in turn, contains a $c$ quark, and is also unstable.}
    \label{fig:b-jet}
  \end{center}
\end{figure}

Flavor-tagging algorithms within ATLAS fall into two categories; those which identify $b$- and $c$-hadrons by their soft lepton decay products, and `lifetime-based' methods which rely on the displacement of the $b$- and $c$-hadron decay products with respect to the primary vertex.
The branching ratio of $c$-hadrons to leptons places an upper limit on the efficiency of soft-lepton based $c$-jet tagging algorithms, which restricts their usefulness for many analyses and as such has not been pursued further at this point.
Lifetime-based methods are generally much more efficient and are well established in $b$-jet identification ($b$-tagging), but so far the extension of these methods to $c$-tagging is less developed.

Lifetime-based tagging can be further divided into two general approaches. The first approach is based on the `impact parameter' (IP) of tracks formed by charged particles in the inner tracking detector with respect to the primary vertex. Impact parameter based tagging algorithms exploit the small IP generally associated to tracks within light-flavored ($u$,$d$,$s$,gluon) jets (light-jets). Tracks from heavy hadron decays, by contrast, generally have a larger IP resulting from the displacement of the decay vertex. The second lifetime-based approach involves explicitly reconstructing at least one `secondary vertex' (SV) from tracks within the jet and categorizing the jet based on the SV properties.

A lifetime-based $c$-tagging algorithm must discriminate against two major backgrounds, $b$- and light-jets, and for most characteristics $c$-jets lie between these two extremes, making isolating
them particularly challenging.
In terms of light-jet discrimination, $c$-tagging algorithms are less powerful than $b$-tagging algorithms as a result of the smaller decay vertex displacement for $c$-hadrons relative to $b$-hadrons\footnote{The mean $c\tau$, where $\tau$ is the particle's lifetime, for a $B$ meson is $\approx 492$~$\mu$m, while $c\tau$ for a $D^{\pm}$ ($D^0$) 
meson is only $\approx 312$ ($\approx 123$) $\mu$m~\cite{pdg2014}.}.
This results in smaller impact parameters and lower SV reconstruction efficiency, and thus in a significantly lower $c$-tagging efficiency to reach a light-jet rejection equal to that achieved by $b$-tagging algorithms.
On the other hand, it can be difficult to distinguish $c$-hadron candidates from $b$-hadrons because both can form a displaced SV
and $b$-hadrons overwhelmingly decay via $c$-hadrons~\cite{pdg2014}. Faced with these complications, the best discriminating variables between $b$- and $c$-jets are the secondary vertex properties, in particular the invariant mass of the charged particles forming the secondary vertex.

Despite the limitations, lifetime-based tagging algorithms are within the capabilities of the detector and require only minor additions to the existing $b$-tagging tools. This chapter describes the design of
one such tagging algorithm, JetFitterCharm, and presents the expected performance and calibration on 2012 data.
For Run 2 the additional new innermost pixel layer inserted into the ATLAS detector during 2014, denoted the Insertable B-Layer~\cite{IBLTDR}, is likely to increase $c$-tagging performance by significantly improving the impact parameter resolution for low momentum tracks.

%------------------- end intro ----------------

%% ------------------------- data and simulation samples --------------------

\section{Data and Simulation Samples}
\label{tag:sec:data-and-simulation}

Plots in this chapter are produced with $\ttbar$ events corresponding to 8 TeV proton-proton collisions simulated with \textsc{Powheg+Pythia6}~\cite{powheg,pythia2} and \textsc{CT10}~\cite{CT10} parton distribution functions. Only $t\bar{t}$ decays with at least one downstream lepton are included. To simulate pileup, minimum bias interactions consistent with 2012 run conditions are generated with \textsc{Pythia8}~\cite{Pythia8} and overlaid on the $\ttbar$ events.
The propagation of particles through the detector and the detector response are modeled using \textsc{GEANT4}~\cite{geant}. The primary vertex is defined as the vertex with the largest sum of squared transverse momenta of the associated tracks.

Jets are reconstructed by clustering energy deposits in the calorimeter with the anti-$k_t$ algorithm~\cite{antikt} and a radius parameter of 0.4, where clusters are calibrated with local cluster weighting~\cite{LCJets}. Jet energy scale calibration and criteria to reject low quality jets are standard across most analyses of 2012 data, and are described in detail elsewhere~\cite{JES}. In this note, only jets with transverse momentum, $\pt$, above 20 GeV and $|\eta| < 2.5$ are considered.
To mitigate effects from pileup, jets with $\pt < 50\,\gev$ and $|\eta| < 2.4$ are rejected if less than half of the sum of track $\pt$ is associated with tracks matched to the primary vertex~\cite{2013JVF}. The same selection cuts are also applied for the data based calibrations presented in the second part of this document.

In the case of simulated jets, a flavor label is assigned by matching jets to generator level partons with $\pt > 5\,\gev$, after final state radiation, in a $\Delta R < 0.3$ cone. If a $b$-quark is found within the cone the jet is labeled as a $b$-jet. If no $b$-quark is found, the search is repeated for $c$-quarks, then for $\tau$ leptons. If no match is found for $b$, $c$, or $\tau$, the jet is labeled as a light-jet.

%% --------------------end data and simulation samples --------------------



\section{Algorithm}
\label{tag:sec:algo}
\input{tex/tagging/algorithm}

\section{Operating Points}

While the neural network produces three posterior probabilities, these sum to approximately 1 and thus contain one redundant degree of freedom. The three outputs are therefore projected into a 2-dimensional `anti-$b$' vs `anti-light' discriminant plane as shown in Fig.~\ref{tag:fig:2dcut}, where the anti-$b$ axis is defined as $\log (P_{c} / P_{b})$ and the anti-light axis as $\log (P_{c} / P_{\rm light})$; the variables are shown in \cref{fig:1dvars1,fig:1dvars2}. Operating points are defined by a pair of minimum thresholds; any jet in which both the anti-light and anti-$b$ discriminants exceed the thresholds is said to be `tagged'.

\begin{figure}
  \begin{center}
  \includegraphics[width=0.9\textwidth]{2d-cut.pdf}
  \caption[JetFitterCharm 2-dimensional cut plane]{
Two-dimensional distribution of the JetFitterCharm anti-$b$ ($\log(P_c/P_b)$) and anti-light ($\log(P_c/P_{\rm light})$) discriminants. The density of red, green, and blue reflect the density of $b$, $c$, and light jets, white areas are a mix of all three flavors, whereas black areas lack any jets. The `medium' calibrated operating point selects jets above a certain threshold in both anti-$b$ and anti-light discriminants.
Ridge structures arise from two features of the algorithm which concentrate values in the input space. The first feature is the use of discrete kinematic bin inputs, $\eta^{\rm cat}$ and $\pt^{\rm cat}$. The second is the substitution of default values when the ordinary input values would not be physically meaningful, for example when no secondary vertex is found. The resulting high density structures can be seen in the lower-center and upper-left region of the plot.}
  \label{tag:fig:2dcut}
  \end{center}
\end{figure}

\begin{figure}
  \begin{center}
    \includegraphics[width=0.6\textwidth]{anti-bottom-discriminant.pdf}
    \caption[JetFitterCharm anti-bottom discriminant]{Distributions of the JetFitterCharm anti-bottom discriminant. The total numbers of jets of each flavor has been normalized to 1. Minimum thresholds for tagged jets are indicated by orange vertical lines. A loose or medium tag requires an anti-bottom discriminant above the threshold.}
  \label{fig:1dvars1}
  \end{center}
\end{figure}

\begin{figure}
  \begin{center}
    \includegraphics[width=0.6\textwidth]{anti-light-discriminant.pdf}
    \caption[JetFitterCharm anti-light discriminant]{Distributions of the JetFitterCharm anti-light discriminant. The total numbers of jets of each flavor has been normalized to 1. Minimum thresholds for tagged jets are indicated by orange vertical lines. A medium tag requires an anti-light discriminant above the threshold.}
  \label{fig:1dvars2}
  \end{center}
\end{figure}

Charm tagging, being relatively new to the LHC, is unable to draw on extensive past experience in physics analyses to define ideal operating points. The operating points were therefore defined with a SUSY $\tilde{t}$ pair production search~\cite{stoptocharm} as a prototype. In this particular model, the $\tilde{t}$ quarks decay via $\tilde{t} \to c \tilde{\chi}_1^0$, leading to a two-$c$-jet final state. Before applying $c$-tagging, expected signal events contribute to the signal region at the percent level, while top backgrounds contribute roughly 10\% and the remaining events are split evenly between $W$ and $Z$ production in association with jets. The anti-$b$ and anti-light tagging thresholds were allowed to vary independently for each of the leading four jets while maximizing the expected signal significance. %% $Z$, where \[Z  \equiv \frac{s}{ \sqrt{s + b + (\Delta b)^2}},\] and $s$ and $b$ are respectively the number of signal and background events passing the tagging requirements. The parameter $\Delta b$ accounts for systematic errors in the background, and is fixed at 0.3.

These studies demonstrated the need for two very different operating points. Anticipating cases where a light jet background dominates, a `medium' tag is defined to reject both light- and $b$-flavored jets. In other selections light jets are a smaller background and $\ttbar$ becomes dominant, so a second `loose' tag is defined which rejects over half of $b$ jets while accepting a larger fraction of $c$-jets. These operating points are summarized in Table~\ref{tab:ops}.

\begin{table}
\begin{center}
\begin{tabular}{c|c c | c c c }
%% \multicolumn{6}{c}{$\charm$-tagging} \\ \hline
Operating Point & $\log (P_c / P_b$) & $\log (P_c / P_{\rm light}$) & $\epsilon_c$ & $1/\epsilon_b$ & $1/\epsilon_{\rm light}$ \\ \hline
loose & $> -0.9$ & -- & 0.95 & 2.5 & 1.0 \\
medium & $> -0.9$ & $> 0.95$ & 0.20 & 8.0 & 200 \\
\end{tabular}
\caption[Charm tagging operating points]{Charm tagging operating points. Charm tagging requires two cuts: one to reject $b$ jets, and another to reject light jets. The approximate efficiencies for light, $c$, and $b$ jets with $\pt > 20\,\text{GeV}$ as estimated on $\ttbar$ events are also given.}
%GP Dan, are the efficiencies estimated on ttbar events?
\label{tab:ops}
\end{center}
\end{table}

%-------------------- performance ----------------

\section{Performance}
\label{sec:ctag-perf}

The $c$-tagging efficiencies as a function of $\pt$ and $|\eta|$ for the medium and loose operating points are shown in \cref{fig:merged-eff}. Beyond the two currently calibrated operating points, many more combinations of anti-$b$ and anti-light thresholds could be useful, depending on a number of analysis-dependent factors such as the number of tags applied and the flavor composition of the dominant backgrounds.

\begin{figure}
  \begin{center}
\includegraphics[width=0.49\textwidth]{ttbar_mc_efficiency_JetFitterCharm20_pt.pdf}
\includegraphics[width=0.49\textwidth]{ttbar_mc_efficiency_JetFitterCharm20_etaAbs.pdf}\\
\includegraphics[width=0.49\textwidth]{ttbar_mc_efficiency_JetFitterCharm90_pt.pdf}
\includegraphics[width=0.49\textwidth]{ttbar_mc_efficiency_JetFitterCharm90_etaAbs.pdf}
\caption{Dependence of the tagging efficiencies on the jet transverse momentum (left) or pseudorapidity (right) for $b$-, $c$-, and light-flavor jets for the JetFitterCharm medium (top) and loose (bottom) operating points. The medium and loose operating points were chosen to give an average $c$-tagging efficiency of $\approx 20\%$ and $\approx 95\%$. \wherefrom}
  \label{fig:merged-eff}
  \end{center}
\end{figure}

\begin{figure}
  \begin{center}
    \includegraphics[width=0.7\textwidth]{jfc-ctag-roc.pdf}
  \caption[Several ROC curves for various $b$-jet rejections]{
    (top) JetFitterCharm light-jet rejection vs $c$-tagging efficiency, where the $b$-rejection ($1/\epsilon_b$) is held fixed. JetFitterCharm operating points select jets above a pair of thresholds in a 2-dimensional discriminant plane, thus for any $c$-tagging efficiency a range of $b$ and light rejections are possible.}
  \label{fig:perf-roc}
  \end{center}
\end{figure}

\begin{figure}
  \begin{center}
    \includegraphics[width=0.7\textwidth]{rejrej-simple.pdf}
  \caption[Light and $b$ rejection for various $c$-tagging efficiencies]{
    Bottom rejection vs. light rejection for constant charm-tagging efficiency. JetFitterCharm operating points select jets above a pair of thresholds in a 2-dimensional discriminant plane, thus for any $c$-tagging efficiency a range of $b$ and light rejections are possible.}
  \label{fig:perf-iso}
  \end{center}
\end{figure}

The range of possible operating points is illustrated in two ways. The first, shown in \cref{fig:perf-roc}, gives the light-jet rejection as a function of $c$-jet efficiency for all possible values of the anti-light threshold ($x$-axis in \cref{tag:fig:2dcut}). In this case, the anti-$b$ threshold is adjusted to maintain a constant $b$-jet rejection. Similar information is given in \cref{fig:perf-iso} in the form of $c$-jet constant efficiency contours in the light- vs $b$-jet rejection plane. Both figures demonstrate the trade off between $b$-jet and light-jet rejection: e.g. for constant 30\% $c$-tagging efficiency an operating point can double its $b$-jet rejection at the expense of cutting the light-jet rejection by a factor of 10.

%% ----------------------------- end performance ---------------

%% ----------------------------- calibration ------------------

\section{JetFitterCharm Calibration}
\label{sec:calib}
Separate analyses are used to calibrate each of the three jet flavors.
The efficiency for tagging $b$-jets is determined from a measurement performed in dileptonic $t\bar{t}$ events with two or three jets, and is based on a combinatorial likelihood approach~\cite{Giacinto}.
The $c$-jet tagging efficiency has been calibrated in multijet events where jets contain $D^*$ mesons~\cite{bc2014}.
Light-jet scale factors are derived from a negative-tag analysis~\cite{bc2014}.
All three calibrations are performed as a function of jet transverse momentum and are provided in terms of scale factors, defined as the ratio of the tagging efficiencies measured in data to those predicted by simulation.
Light-jet scale factors are further binned as a function of $|\eta|$. No $\eta$-binning is used for $b$- and $c$-jet scale factors, as these scale factors have no significant dependence on jet $\eta$.
Figures~\ref{JFC_SF_B} and~\ref{JFC_SF_L1} give scale factors for all jets at the medium operating point. Figures~\ref{JFC_SF_B_loose} and~\ref{JFC_SF_L1_loose} give the same information for the loose operating point.

\newcommand{\lSF}{as detailed in~\cite{bc2014}. The scale factors are measured relative to dijet Pythia8+EvtGen} %\ref{lSF}
\newcommand{\cSF}{as detailed in~\cite{bc2014}. The scale factors are measured relative to dijet Pythia8} %\ref{cSF}
\newcommand{\bSF}{as detailed in~\cite{Giacinto}. The scale factors are measured relative to $t \bar{t}$ Powheg+Pythia6} %\ref{clSF}
\newcommand{\bcSF}{The derivation of the $b$-tagging ($c$-tagging) scale factors, shown here relative to $t \bar{t}$ Powheg+Pythia6 (dijet Pythia8), is detailed in~\cite{Giacinto} (\hspace{1sp}\cite{bc2014})}

\begin{figure}
  \centering
    \includegraphics[width=0.49\textwidth]{%
SFs_internal_nomistag/JetFitterCharm_medium_C_default_SF_oldcalib_000_250.pdf}
  \includegraphics[width=0.49\textwidth]{%
SFs_internal_nomistag/JetFitterCharm_medium_B_default_SF_000_250.pdf}
  \caption{Dependence of the $c$-jet (left) and $b$-jet (right) efficiency scale factors on the jet transverse momentum for the medium operating point of the
    JetFitterCharm tagging algorithm. \bcSF.}
  \label{JFC_SF_B}
\end{figure}

\begin{figure}
  \centering
    \includegraphics[width=0.49\textwidth]{%
SFs_internal_nomistag/JetFitterCharm_medium_Light_default_SF_000_120.pdf}
  \includegraphics[width=0.49\textwidth]{%
SFs_internal_nomistag/JetFitterCharm_medium_Light_default_SF_120_250.pdf}
  \caption{Dependence of the light-jet efficiency scale factor on the jet transverse momentum, for jet pseudorapidity $0.0 < | \eta | < 1.2$ (left) or 
    $1.2 < | \eta | < 2.5$ (right), for the medium operating point of the  JetFitterCharm tagging algorithm, \lSF.}
  \label{JFC_SF_L1}
\end{figure}


\begin{figure}
  \centering
  \includegraphics[width=0.49\textwidth]{%
SFs_internal_nomistag/JetFitterCharm_loose_C_default_SF_oldcalib_000_250.pdf}
  \includegraphics[width=0.49\textwidth]{%
SFs_internal_nomistag/JetFitterCharm_loose_B_default_SF_000_250.pdf}
  \caption{Dependence of the $c$-jet (left) and $b$-jet (right) efficiency scale factors on the jet transverse momentum for the loose operating point of the 
    JetFitterCharm tagging algorithm. \bcSF.}
  \label{JFC_SF_B_loose}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.49\textwidth]{%
SFs_internal_nomistag/JetFitterCharm_loose_Light_default_SF_000_120.pdf}
  \includegraphics[width=0.49\textwidth]{%
SFs_internal_nomistag/JetFitterCharm_loose_Light_default_SF_120_250.pdf}
  \caption{Dependence of the light-jet efficiency scale factor on the jet transverse momentum, for jet pseudorapidity $0.0 < | \eta | < 1.2$ (left) or 
    $1.2 < | \eta | < 2.5$ (right), for the loose operating point of the JetFitterCharm tagging algorithm, \lSF. The total uncertainty is constrained to give a maximum corrected efficiency of 1.}
  \label{JFC_SF_L1_loose}
\end{figure}

\clearpage

%------------------- end calibration ----------------

%% ------------------ future of c-tagging ------------------

\section{The Future of Flavor Tagging}
JetFitterCharm, while effective, is plagued by several related issues.
First, JetFitterCharm's $b$-vs-light discrimination is inferior to that from MV1, the baseline $b$-tagger in run 1. This discrepancy is illustrated in \cref{fig:u-rej-roc}, and is surprising given that both are neural nets with roughly the same inputs.
In addition, the neural net training isn't reproducible; retraining on exactly the same dataset may produce performance that's better or worse by several percent.
Both issues stem from the unreliable methods which are typically used to train neural nets.

\begin{figure}
  \begin{center}
    \includegraphics[width=0.7\textwidth]{figures/external/uRejRoc.pdf}
    \caption[Light jet rejection as a function of $b$-tagging efficiency]{Light jet rejection vs $b$-tagging efficiency for several taggers. The MV1 tagger (labeled \plt{mv1}) was the baseline $b$-tagger for Run 1. Both \plt{gaiaAntiU} and \plt{jfcAntiU} (from  GAIA and JetFitterCharm respectively) rely on neural nets with multiple outputs and use the $\log (P_{b} / P_{\text{light}})$ ratio as a discriminant. A final tagger \plt{gaiaGr1}, is tuned to reject both $c$ and light jets, and discriminates with $\log [P_{b}^2 / (P_{c} P_{\text{light}})]$.  The bottom panel shows relative efficiency vs the \plt{gaiaGr1} baseline.}
    \label{fig:u-rej-roc}
  \end{center}
\end{figure}

\subsection{Traditional Neural Networks}
\label{sec:trad-nn}
Traditional signal-processing neural nets are continuous mappings of input vectors $\vect{x}$ onto output vectors $\vect{y}$, where both $\vect{x}$ and $\vect{y}$ can be of arbitrary dimension for a given neural net.
An $N + 1$ layer neural net can be represented as stack of linear transformations $A_{i}$ interleaved with a nonlinear activation function $f$:
\begin{equation}
  \vect{y} = \mathcal{N}(\vect{x}) = A_N f(A_{N -1} \cdots f(A_1 \vect{x} + \vect{b}_1) \cdots + \vect{b}_{N-1} ) + \vect{b}_N
  \label{eq:nn-functional}
\end{equation}
where the $\vect{b}_i$ are \emph{bias} vectors and $f$ operates on each element of its input vector independently.
A typical activation function is the sigmoid $f(x_j) = (1 + e^{-x_j})^{-1}$.
In \cref{eq:nn-functional}, $\vect{x}$ and $\vect{y}$ are referred to as the input and output layers, respectively.
For historical reasons, the elements of $A_i$ are also known as synapse weights, while the elements of $\vect{b}_i$ are known as activation thresholds.
The inputs to each $A_i$ (other than $A_1$) are referred to as \emph{hidden} layers $\vect{z_i}$. To make the hidden layers more explicet \cref{eq:nn-functional} can also be written recursively:
\begin{equation}
  \vect{z}_{i+1} = f(A_{i} \vect{z}_{i} + \vect{b}_{i}),
\end{equation}
where $\vect{y} = \vect{z}_{N+1}$ and $\vect{x} = \vect{z}_{1}$.
In some cases the activation function is omitted for the final layer, as in \cref{eq:nn-functional}

Note that the dimensions of the $A_{i}$ and $\vect{b}_i$ are constrained by the dimensions of the surrounding layers; if $\vect{x}$ has $n$ elements, $A_1$ is an $m \times n$ matrix, $A_{2}$ is an $o \times m$ matrix, $A_{3}$ is $p \times o$, etc.
Thus given some number of inputs and outputs, the neural net is still left with $N - 1$ free dimensions---one for each hidden layer.
The dimensions of the input, output, and hidden layers specify the \emph{topology} of the neural net.
These dimensions typically decrease through successive layers, since the neural net is designed to filter a large number of parameters into a smaller number of categories.
Traditional neural nets can include several hidden layers; those with more layers are described as ``deep''.
Theoretically a deep neural net can fit a continuous function of $\vect{x}$ to arbitrary precision given an appropriate choice of the of $A_i$ and $\vect{b}_i$.

In the case of flavor tagging, the elements of $\vect{x}$ correspond to measurable jet parameters, while the elements $\vect{y}$ correspond to jet flavor probabilities. Elements of $\{(A_i, \vect{b}_i)\}_{i=0}^{N}$ are ``trained'' using inputs with corresponding \emph{target} outputs $\vect{t}$, such that $\mathcal{N}(\vect{x}) \approx \vect{t}$.
Within flavor tagging these targets correspond to jet flavor probabilities, with one element for each flavor.
This training scheme is said to be \emph{supervised} because it requires known target values.

Traditionally, training progresses in several steps using a technique known as error backpropagation. The steps are as follows:
\begin{enumerate}
\item An activation function $f$ and topology are chosen.
\item The neural net is initialized with random synapse weights and activation thresholds.
\item A pattern is selected from a \emph{training sample} which includes the input values $\vect{x}$ and the \emph{target} values $\vect{t}$. The value of $\vect{y}$ is computed using the current $A_i$ and $\vect{b}$ parameters. \label{item:nn-pattern-select}
\item An \emph{cost function} $E(\vect{y},\vect{t})$ quantifies the amount by which the pattern is misclassified.
A particularly simple cost function is $E(\vect{y},\vect{t}) = \sum_{i} (y_i - t_i)^2 / 2$. %The error $\vect{e}$ on $\vect{y}$ is defined as
%% \begin{equation}
%% \end{equation}
The cost function determines an error for each output element.
\item The error is backpropagated to the previous (hidden) layer using the $A_i$ matrix. By recursively applying backpropagation to each successive layer an error can be established for each element in each hidden layer~\cite{luke-thesis}.
\item The values of $A_i$ and $\vect{b}_i$ are updated by moving both in a direction that will decrease the cost function. The size of the update is dictated by the \emph{learning rate}.\label{item:nn-update}
\item Steps~\ref{item:nn-pattern-select} through~\ref{item:nn-update} are repeated until the cost is minimized.
\end{enumerate}

This procedure works well for shallow networks with one or two hidden layers.
In deeper networks the backpropagation procedure begins to break down.
Backpropagation is a useful heuristic, but doesn't guarantee rapid convergence, especially when errors are propagated through several hidden layers.
As a result training can be prohibitively slow on deeper networks.
Furthermore, once the cost function is minimized there's no guarantee that the neural net has found the global minimum.
Autoencoders solve these problems, as discussed in the next section.

\subsection{Autoencoders}

The conceptual key to autoencoders is recognition that hidden layers are more than placeholders in a numeric filtering sequence.
Instead, they are cast as \emph{feature spaces} which describe the characteristics of input patterns in terms of a reduced set of parameters.
In this view, each layer of a neural net is a compression; it reduces the dimension of the input space while preserving the important information.
The next important principle is that feature spaces can be agnostic to the target patterns on which the neural net is trained or, in the flavor-tagging context, agnostic to the jet flavor.
These two ideas motivate a completely new approach to training neural nets, which proceeds as follows:
\begin{enumerate}
\item A three-layer neural net is defined, with the structure
  \begin{equation}
    \tilde{\vect{x}} = D f(A_1 \vect{x} + \vect{b}_1) + \vect{c}.
    \label{eq:autoencoder-1st}
  \end{equation}
  As above, the matrices and bias vectors are initialized randomly. The matrix $A_1$ is an $m \times n$ matrix with $m < n$, such that it reduces the dimension of the input vector.
\item The neural net is trained with the aim that $\tilde{\vect{x}} \approx \vect{x}$, using steps~\ref{item:nn-pattern-select}--\ref{item:nn-update} from the traditional backpropagation above.
At this point it's convenient to rewrite \cref{eq:autoencoder-1st} as
\begin{equation}
    \tilde{\vect{x}} = D \vect{z}_2 + \vect{c}
\end{equation}
using the above definition of $\vect{z}_i$.
The content of $A_1$ and $b_1$ describe a (lossy) \emph{encoding} of the input space, such that $\vect{z}_2$ is a lower-dimensional representation of $\vect{x}$.
Applying $D$ and $\vect{c}$ decodes the compressed $\vect{z}_2$ and reconstructs the original inputs. Note that this only requires input patterns; the training sequence is \emph{unsupervised} because no traditional target values are required.

\item Both $D$ and $\vect{c}$ are discarded, and a new neural net is defined:
  \begin{equation}
    \tilde{\vect{z}_2} = D f(A_2 \vect{z_2} + \vect{b}_2) + \vect{c}
    \label{eq:autoencoder-2st}
  \end{equation}
  where $\vect{z}_2 \equiv f(A_1 \vect{x} + \vect{b}_1)$, and $A_2$ is a dimension-reducing transform as before. Again the neural net is trained with backpropagation, ignoring the first layer and focusing only on the second.
\label{item:autoencoder-layer}
\item Step~\ref{item:autoencoder-layer} is repeated as many times as desired. The resulting stack of encoding layers transforms an input into low-dimensional representation.
\item As a result of the unsupervised training, the final feature space is unlikely to be aligned with the actual target values. Within the flavor tagging context jets are clumped in features which may reflect flavor, but which aren't aligned with the flavor basis.
A final \emph{fine tuning} step aligns these features in the correct basis using the full backpropagation sequence described in \cref{sec:trad-nn}.
\end{enumerate}
This training sequence can is orders of magnitude faster and far more robust than traditional backpropagation.
It is therefore able to construct more complicated neural nets which squeeze much more discrimination out of the input variables.

\subsection{Performance Comparisons}

Explicit proof are few and far between in machine learning---what counts as ``proof'' is generally empirical.
In the case of flavor-tagging, the improvement due to autoencoders is demonstrated in simulated data, with various metrics depending on the tagged flavor.
In the performance comparisons that follow, we compare several new taggers: MV1 was the baseline $b$-tagging tool for Run 1 analyses; GAIA is a new autoencoder-based neural network.
Like JetFitterCharm, GAIA includes three outputs, and discriminates signal from background by selecting jets above a $\log(P_{\text{signal}} / P_{\text{background}})$ threshold.

\subsubsection{$b$ tagging}
In the $b$-tagging case, performance is generally quantified with a Receiver Operating Characteristic (ROC) curve, which is produced by plotting light-jet rejection vs $b$-jet efficiency improvement for a range of selections. Each selection is defined by a specific threshold, over which all jets are accepted.
A comparison on ROC curves for various taggers is given in \cref{fig:u-rej-roc}.
Clearly the GAIA's deep architecture outperforms MV1 when configured as an anti-light tagger using the $\log(P_{b}/P_{\text{light}})$ discriminant.

\begin{figure}
  \begin{center}
    \includegraphics[width=0.7\textwidth]{figures/external/cRejRoc.pdf}
    \caption[Charm jet rejection as a function of $b$-tagging efficiency]{Charm jet rejection vs $b$-tagging efficiency for several taggers. The MV1 tagger (labeled \plt{mv1}) was the baseline $b$-tagger for Run 1. Both \plt{gaiaAntiU} and \plt{jfcAntiU} (from  GAIA and JetFitterCharm respectively) rely on neural nets with multiple outputs and use the $\log (P_{b} / P_{\text{light}})$ ratio as a discriminant. A final tagger \plt{gaiaGr1}, is tuned to reject both $c$ and light jets, and discriminates with $\log [P_{b}^2 / (P_{c} P_{\text{light}})]$.  The bottom panel shows relative efficiency vs the \plt{gaiaGr1} baseline.}
    \label{fig:c-rej-roc}
  \end{center}
\end{figure}

Charm jets can also become a major background in a number of searches.
\Cref{fig:c-rej-roc} gives ROC curves for $c$-jet rejection.
In this case the anti-light GAIA configuration performs on par with MV1, which is unsurprising given that the MV1 training ignores $c$-jets and the GAIA anti-light discriminant ignores $P_{c}$.
To reject a higher portion of $c$-jets, GAIA uses a mixed discriminant of the form $\log[P_{b}^2/(P_{\text{light}} P_{c})]$.
This discriminant rejects far more $c$-jets for a given efficiency, at the price of slightly lowered light-jet rejection.

All lifetime-based taggers perform best on jets within a specific $\pt$ range.
Performance increases steadily with $\pt$ up to roughly $100\,\gev$, after which it begins to decrease.
Poor performance at low $\pt$ is expected, since less boosted objects will decay closer to the primary vertex and secondary vertex reconstruction will fail.
The loss in performance at higher pt, by contrast, is more difficult to explain---while tracks are more colinear (and thus less able to form a well-defined vertex) they should also travel further before decaying (and thus have a more apparent displaced vertex).
The loss of high-$\pt$ performance may simply be an artifact of training on $t\bar{t}$ samples: the vast majority of jets in these samples have $\pt < 200\,\gev$.

\begin{figure}
  \includegraphics[width=0.5\textwidth]{figures/external/uRej70_ptbins.pdf}
  \includegraphics[width=0.5\textwidth]{figures/external/cRej70_ptbins.pdf}
  \caption[Rejection vs $\pt$ for $b$-taggers]{Rejection vs $\pt$ for $b$-taggers. Tagging thresholds are adjusted individually for jets in each $\pt$ bin, such that 70\% of jets are tagged in each bin. The MV1 tagger (labeled \plt{mv1}) was the baseline $b$-tagger for Run 1. Both \plt{gaiaAntiU} and \plt{jfcAntiU} (from  GAIA and JetFitterCharm respectively) rely on neural nets with multiple outputs and use the $\log (P_{b} / P_{\text{light}})$ ratio as a discriminant. A final tagger \plt{gaiaGr1}, is tuned to reject both $c$ and light jets, and discriminates with $\log [P_{b}^2 / (P_{c} P_{\text{light}})]$. }
  \label{fig:rej70-ptbins}
\end{figure}

\subsubsection{$c$ tagging}

As discussed in \cref{sec:ctag-perf}, $c$-tagging performance metrics are inherently more complicated than those used in $b$ tagging.
To compare the $c$-tagging performance across multiple taggers we calculate the relative $c$-tagging efficiency at any given $b$ and light rejection, as shown in \cref{fig:ctag-gaia-vs-jfc}.
At any chosen point, GAIA outperforms JetFitterCharm.

\begin{figure}
  \begin{center}
  \includegraphics[width=0.8\textwidth]{%
    figures/external/ctag-2d-gaia-vs-jfc.pdf}
  \caption[Charm jet efficiency comparison in duel-rejection plane]{%
    Relative efficiency of GAIA / JetFitterCharm in the light-vs-$b$ rejection plane. Black thick black contours indicate constant efficiency for GAIA.}
  \label{fig:ctag-gaia-vs-jfc}
  \end{center}
\end{figure}

%% ------------------------ end future of c-tagging ----------------------

\section{Discriminant Dimensionality}

As discussed above, $b$-tagging discriminants are typically 1-dimensional, whereas $c$-tagging discriminants are 2-dimensional.
A 2-dimensional $b$-tagging discriminant obviously has advantages; as shown in \cref{fig:u-rej-roc,fig:c-rej-roc,fig:rej70-ptbins} various combinations of the GAIA output probabilities can trade light for $c$ rejection, similar to the $b$-vs-$c$ rejection trade-offs within $c$-tagging. The potential range of trad-offs is much smaller than that which is available to a $c$ tagger.
This is illustrated in \cref{fig:btag-vs-ctag-rejrej}, where $b$ tagger clearly has a much narrower range of useful rejections available for a given $c$-tagging efficiency.

\begin{figure}
  \includegraphics[width=0.5\textwidth]{figures/external/rejrej-btag.pdf}
  \includegraphics[width=0.5\textwidth]{figures/external/rejrej-cprob.pdf}
  \caption[Rejection Trade-offs for $b$ and $c$ tagging]{%
    Comparison of possible rejection trade-offs for $b$-tagging (left) and $c$-tagging (right).
The black lines are iso-efficiency contours for GAIA.
A much smaller range of rejection trade-offs are available for a given $b$-tagging efficiency. The right plot is overlaid with the light vs $b$ rejections for a 1-dimensional $c$-tagging discriminant \plt{gaia 1D}. Large red dots on this line represent points where the 1-dimensional discriminant crosses the efficiency given by the nearby iso-efficiency contour.}
  \label{fig:btag-vs-ctag-rejrej}
\end{figure}

%% It's also reasonable to question the 2-dimensional requirement for  \cref{fig:btag-vs-ctag-rejrej} is the converse 
